{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erika-Dunn/IEP-RAG-IEP-Assistant/blob/main/FINAL_NLP_Course_CLEAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxHCm9n2DIEd"
      },
      "source": [
        "# Section 0: Prepping for GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcTYx5SMDNte"
      },
      "outputs": [],
      "source": [
        "# Set Git identity\n",
        "!git config --global user.name \"Erika Kelly\"\n",
        "!git config --global user.email \"erika.l.dunn@gmail.com\"\n",
        "\n",
        "# Clone your GitHub repo\n",
        "!git clone https://github.com/Erika-Dunn/IEP-RAG-IEP-Assistant.git\n",
        "\n",
        "# Change directory\n",
        "%cd IEP-RAG-IEP-Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xTaWpX9eaOd"
      },
      "source": [
        "\n",
        "\n",
        "# Section 1: Data Collection and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP10YJoUeNFL"
      },
      "outputs": [],
      "source": [
        "# Install Required Packages\n",
        "\n",
        "!pip install -q faiss-cpu sentence-transformers pdfplumber beautifulsoup4 requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUhobOOce1vs"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "from google.colab import drive\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pdfplumber\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import faiss\n",
        "import pickle\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ02Yy3Eyu6s"
      },
      "outputs": [],
      "source": [
        "# Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C19elsrzu4N"
      },
      "outputs": [],
      "source": [
        "# Load BLS OOH XML and chunk\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/NLP Course/ooh_occupations.xml'\n",
        "tree = ET.parse(file_path)\n",
        "root = tree.getroot()\n",
        "\n",
        "def extract_text(el):\n",
        "    return ''.join(el.itertext()).strip() if el is not None else None\n",
        "\n",
        "data = []\n",
        "for occ in root.findall('.//occupation'):\n",
        "    data.append({\n",
        "        'job_code': extract_text(occ.find('occupation_code')),\n",
        "        'job_title': extract_text(occ.find('title')),\n",
        "        'what_they_do': extract_text(occ.find('what_they_do') or occ.find('summary_what_they_do')),\n",
        "        'work_environment': extract_text(occ.find('work_environment') or occ.find('summary_work_environment')),\n",
        "        'how_to_become': extract_text(occ.find('how_to_become_one') or occ.find('summary_how_to_become_one')),\n",
        "        'pay': extract_text(occ.find('pay') or occ.find('summary_pay')),\n",
        "        'job_outlook': extract_text(occ.find('job_outlook') or occ.find('summary_outlook')),\n",
        "        'similar_occupations': extract_text(occ.find('similar_occupations') or occ.find('summary_similar_occupations'))\n",
        "    })\n",
        "\n",
        "ooh_df = pd.DataFrame(data)\n",
        "ooh_chunks = []\n",
        "for _, row in ooh_df.iterrows():\n",
        "    for sec in ['what_they_do', 'work_environment', 'how_to_become', 'pay', 'job_outlook', 'similar_occupations']:\n",
        "        text = row[sec]\n",
        "        if pd.notna(text) and text.strip():\n",
        "            ooh_chunks.append({\n",
        "                'source': 'bls_ooh',\n",
        "                'section': sec,\n",
        "                'text': re.sub(r'\\s+', ' ', text.strip()),\n",
        "                'job_code': row['job_code'],\n",
        "                'job_title': row['job_title']\n",
        "            })\n",
        "\n",
        "# Oregon IEP - Web and PDF\n",
        "url = \"https://www.oregon.gov/ode/students-and-family/SpecialEducation/publications/Pages/Oregon-Standard-IEP.aspx\"\n",
        "soup = BeautifulSoup(requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}).text, \"html.parser\")\n",
        "web_lines = [line.strip() for line in soup.get_text(\"\\n\").splitlines() if len(line.strip()) > 30]\n",
        "web_chunks = [{\"source\": \"oregon_iep_web\", \"section\": f\"paragraph_{i+1}\", \"text\": line, \"job_code\": None, \"job_title\": None}\n",
        "              for i, line in enumerate(web_lines) if len(line.split()) > 10]\n",
        "\n",
        "pdf_links = [a['href'] if a['href'].startswith('http') else f\"https://www.oregon.gov{a['href']}\"\n",
        "             for a in soup.find_all(\"a\", href=True) if '.pdf' in a['href'].lower()]\n",
        "\n",
        "os.makedirs(\"oregon_iep_pdfs\", exist_ok=True)\n",
        "pdf_chunks = []\n",
        "for i, link in enumerate(pdf_links):\n",
        "    filename = f\"oregon_iep_pdfs/doc_{i+1}.pdf\"\n",
        "    r = requests.get(link)\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "    with pdfplumber.open(filename) as pdf:\n",
        "        for j, page in enumerate(pdf.pages):\n",
        "            txt = page.extract_text() or \"\"\n",
        "            for k, chunk in enumerate(txt.split(\"\\n\\n\")):\n",
        "                if len(chunk.split()) > 10:\n",
        "                    pdf_chunks.append({\n",
        "                        \"source\": \"oregon_iep_pdf\",\n",
        "                        \"section\": f\"doc_{i+1}_pg{j+1}_blk{k+1}\",\n",
        "                        \"text\": chunk.strip(),\n",
        "                        \"job_code\": None,\n",
        "                        \"job_title\": None\n",
        "                    })\n",
        "\n",
        "# NASET IEP PDF Chunking\n",
        "naset_path = \"/content/drive/MyDrive/Colab Notebooks/NLP Course/Completed_Sample_IEP.pdf\"\n",
        "with pdfplumber.open(naset_path) as pdf:\n",
        "    naset_text = \"\\n\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
        "\n",
        "naset_chunks = []\n",
        "raw_chunks = re.split(r'(Measurable Postsecondary Goal.*?:|Annual Goal.*?:|Short[- ]Term Objective.*?:)', naset_text)\n",
        "for i in range(1, len(raw_chunks), 2):\n",
        "    label = raw_chunks[i].strip()\n",
        "    content = raw_chunks[i+1].strip()\n",
        "    if len(content.split()) >= 10:\n",
        "        naset_chunks.append({\n",
        "            \"source\": \"naset_iep_example\",\n",
        "            \"section\": label,\n",
        "            \"text\": content,\n",
        "            \"job_code\": None,\n",
        "            \"job_title\": None\n",
        "        })\n",
        "\n",
        "# Merge all chunks into one DF\n",
        "combined_df = pd.DataFrame(ooh_chunks + web_chunks + pdf_chunks + naset_chunks)\n",
        "\n",
        "# Embedding + FAISS\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "texts = combined_df['text'].tolist()\n",
        "embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "faiss.normalize_L2(embeddings)\n",
        "\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "faiss.write_index(index, \"/content/drive/MyDrive/Colab Notebooks/NLP Course/full_rag.index\")\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/NLP Course/full_rag_metadata.pkl\", \"wb\") as f:\n",
        "    pickle.dump(combined_df.to_dict(orient='records'), f)\n",
        "\n",
        "print(f\"✅ Combined RAG index created with {len(combined_df)} records.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lm6llCMo2QZ"
      },
      "source": [
        "# Section 2: RAG Pipeline Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "777bc100"
      },
      "source": [
        "# Section 2 · RAG Retrieval + Generation (Zephyr 7B, FAISS, Colab Pro)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4ce0424"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers faiss-cpu transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23247d13"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, pickle, torch\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b3138f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Config\n",
        "INDEX_PATH     = \"/content/drive/MyDrive/Colab Notebooks/NLP Course/full_rag.index\"\n",
        "META_PATH      = \"/content/drive/MyDrive/Colab Notebooks/NLP Course/full_rag_metadata.pkl\"\n",
        "EMBED_MODEL    = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "LLM_NAME       = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "RETRIEVE_K     = 3\n",
        "MAX_NEW_TOKENS = 300\n",
        "DEVICE         = 0 if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abdd94b5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load FAISS index and metadata\n",
        "faiss_index = faiss.read_index(INDEX_PATH)\n",
        "doc_meta    = pickle.load(open(META_PATH, \"rb\"))\n",
        "print(f\"✅ FAISS index loaded with {faiss_index.ntotal} vectors\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bd09450"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Embed query + retrieve top K\n",
        "def retrieve(query: str, k: int = RETRIEVE_K, embedder=None):\n",
        "    embedder = embedder or SentenceTransformer(EMBED_MODEL)\n",
        "    q_vec = embedder.encode(query, normalize_embeddings=True)\n",
        "    D, I  = faiss_index.search(q_vec.reshape(1, -1).astype(\"float32\"), k)\n",
        "    return [dict(doc_meta[i], sim=float(score)) for score, i in zip(D[0], I[0])]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1c992b7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Prompt builder\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are an educational planning assistant. \"\n",
        "    \"Use only the CONTEXT provided. Cite facts as [SOURCE:<section>].\"\n",
        ")\n",
        "\n",
        "def make_prompt(question: str, docs: list[dict]) -> str:\n",
        "    context = \"\\n\\n---\\n\".join(\n",
        "        f\"{doc['text']} [SOURCE:{doc['section']}]\"\n",
        "        for doc in docs\n",
        "    )\n",
        "    return f\"<|system|>\\n{SYSTEM_PROMPT}\\n<|user|>\\nCONTEXT:\\n{context}\\n\\nQUESTION:\\n{question}\\n<|assistant|>\\n\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9a09d06"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load Zephyr-7B safely on GPU (if available)\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=AutoModelForCausalLM.from_pretrained(\n",
        "        LLM_NAME,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True\n",
        "    ),\n",
        "    tokenizer=AutoTokenizer.from_pretrained(\n",
        "        LLM_NAME,\n",
        "        use_fast=True,\n",
        "        trust_remote_code=True\n",
        "    ),\n",
        "    max_new_tokens=MAX_NEW_TOKENS\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcbfa2f7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# RAG pipeline with token-safe truncation and attention mask\n",
        "def rag_chat(question: str, k: int = RETRIEVE_K, max_new_tokens: int = MAX_NEW_TOKENS):\n",
        "    hits = retrieve(question, k=k)\n",
        "    prompt = make_prompt(question, hits)\n",
        "\n",
        "    tokenizer = generator.tokenizer\n",
        "    model_max_len = 8192\n",
        "    input_max_len = model_max_len - max_new_tokens\n",
        "\n",
        "    encodings = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n",
        "                          padding=True, max_length=input_max_len)\n",
        "    input_ids = encodings.input_ids.to(generator.model.device)\n",
        "    attention_mask = encodings.attention_mask.to(generator.model.device)\n",
        "\n",
        "    output_ids = generator.model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    answer = output.split(\"<|assistant|>\")[-1].strip()\n",
        "    return answer, hits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WICWQiu5diu6"
      },
      "source": [
        "# Section 3: Prompt Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wqf2Kkj46mNU"
      },
      "outputs": [],
      "source": [
        "# RAG Goal Generation Pipeline – Gradio-Ready\n",
        "\n",
        "# --- Prompt Builders ---\n",
        "\n",
        "def extract_student_info_prompt(profile_text):\n",
        "    return f\"\"\"Extract the following structured information from the student's profile text below.\n",
        "If information is not present, write \"missing\". Output in JSON format with these fields:\n",
        "- name\n",
        "- age\n",
        "- grade level\n",
        "- disability\n",
        "- strengths\n",
        "- academic concerns\n",
        "- support needs\n",
        "- postsecondary goal (employment)\n",
        "- postsecondary goal (education)\n",
        "- postsecondary goal (independent living)\n",
        "\n",
        "Student profile:\n",
        "\\\"\\\"\\\"{profile_text}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "def generate_goals_prompt(structured_profile_json):\n",
        "    return f\"\"\"Based on the student's profile and postsecondary employment and education goals,\n",
        "write three SMART annual IEP goals — one for each of the following areas: academic achievement,\n",
        "independent living skills, and career preparation. Goals must follow this structure:\n",
        "\n",
        "[Condition], [Student] will [behavior] [criteria] [timeframe].\n",
        "\n",
        "Use the following profile:\n",
        "{structured_profile_json}\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are an educational planning assistant. \"\n",
        "    \"Use only the CONTEXT provided. Cite facts as [SOURCE:<section>].\"\n",
        ")\n",
        "\n",
        "def make_prompt(question: str, docs: list[dict]) -> str:\n",
        "    context = \"\\n\\n---\\n\".join(\n",
        "        f\"{doc['text']} [SOURCE:{doc['section']}]\" for doc in docs\n",
        "    )\n",
        "    return f\"<|system|>\\n{SYSTEM_PROMPT}\\n<|user|>\\nCONTEXT:\\n{context}\\n\\nQUESTION:\\n{question}\\n<|assistant|>\\n\"\n",
        "\n",
        "# --- Mock LLM + Search (Replace in production) ---\n",
        "\n",
        "def llm(prompt: str) -> dict:\n",
        "    print(\"LLM PROMPT:\")\n",
        "    print(prompt)\n",
        "    return {\n",
        "        \"employment_goal\": \"After high school, Clarence will obtain a full-time job at Walmart as a sales associate.\",\n",
        "        \"education_goal\": \"After high school, Clarence will complete on-the-job training provided by Walmart and participate in employer-sponsored customer service workshops.\",\n",
        "        \"annual_goal\": \"In 36 weeks, Clarence will demonstrate effective workplace communication and customer service skills...\",\n",
        "        \"alignment\": [\"OOH standards for Retail Sales Workers\", \"21st Century Skills\"],\n",
        "        \"benchmarks\": [\n",
        "            \"Greet customers appropriately\",\n",
        "            \"Maintain eye contact\",\n",
        "            \"Listen actively\",\n",
        "            \"Respond to customer questions\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "def vector_search(query: str) -> list:\n",
        "    return [\n",
        "        {\"text\": \"Retail sales roles require strong communication, customer service, and product knowledge.\", \"section\": \"OOH-Retail\"},\n",
        "        {\"text\": \"21st Century standards emphasize collaboration, communication, and critical thinking.\", \"section\": \"OR-21stCentury\"}\n",
        "    ]\n",
        "\n",
        "# --- Main Processing Pipeline ---\n",
        "\n",
        "def process_student_profile(profile_text: str):\n",
        "    # Step 1: Extract structured fields\n",
        "    extraction_prompt = extract_student_info_prompt(profile_text)\n",
        "    structured_info = llm(extraction_prompt)\n",
        "\n",
        "    # Step 2: Search vector DB using postsecondary employment goal\n",
        "    goal_query = structured_info.get(\"postsecondary goal (employment)\", \"undecided\")\n",
        "    docs = vector_search(goal_query)\n",
        "\n",
        "    # Step 3: Generate SMART goal prompt\n",
        "    question = generate_goals_prompt(structured_info)\n",
        "    full_prompt = make_prompt(question, docs)\n",
        "\n",
        "    # Step 4: Generate final goals\n",
        "    return llm(full_prompt)\n",
        "\n",
        "# --- Example Run (Clarence Case Study) ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    clarence_case_study = \"\"\"\n",
        "    Clarence is a 15-year-old sophomore with a behavior disorder.\n",
        "    He completed the O*Net Interest Profiler and showed strong interest in the 'Enterprising' category.\n",
        "    Career interests include retail sales and driver/sales worker.\n",
        "    Clarence prefers hands-on learning over academic instruction.\n",
        "    He expressed in his Vision for the Future interview that he would like to work at Walmart.\n",
        "    \"\"\"\n",
        "    results = process_student_profile(clarence_case_study)\n",
        "    for k, v in results.items():\n",
        "        print(f\"{k.upper()}:\\n{v}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}